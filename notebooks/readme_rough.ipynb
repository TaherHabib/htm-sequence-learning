{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0396a325",
   "metadata": {},
   "source": [
    "# Implicit Acquisition of Simple Reber Grammar (SRG) in a Neocortex-inspired Neural Network\n",
    "(My master's thesis' code repository)\n",
    "\n",
    "## Project Summary\n",
    "The human brain seems to acquire sequential information – mainly, recognition and prediction of temporally-correlated patterns – almost seamlessly. Moreover, the acquisition is even implicit in some cases like language. In my thesis, I attempt to study this feature of the biological brain by addressing the question of implicit (unsupervised) learning of nontrivial and higher-order sequential information over time.\n",
    "\n",
    "I use Numenta’s [Hierarchical Temporal Memory (HTM) (2016)](https://numenta.com/neuroscience-research/research-publications/papers/why-neurons-have-thousands-of-synapses-theory-of-sequence-memory-in-neocortex/) – which deploys several architectural and functional features of the neocortex in addition to utilizing Hebbian plasticity based learning techniques. The task used in the experiments is to implicitly learn higher-order temporal dependencies in sentences (strings) generated from an artificial grammar, that is known to model the implicit acquisition mechanisms of language processing in the human brain.\n",
    "\n",
    "## Repository Structure\n",
    "- `experiment_modules/` directory contains python modules defining interfaces for an HTM cell, an HTM network, a Reber Grammar Generator and an Experimentor; along with all their corresponding functions used to perform different aspects of the experiment.\n",
    "- `notebooks/` directory contains the notebooks showcasing the diffent experiments in my thesis, as follows:\n",
    "    - implicit learning of a single Reber String (e.g. _PVPVZ_) by the HTM network, \n",
    "    - learning of long-range dependencies, \n",
    "    - implicitly learning SRG with pruning of dendrites based on `maxDendriteDormancy` parameter,\n",
    "    - implicitly learning SRG without pruning of dendrites.\n",
    "- `results/` directory contains some image files displaying network states and predictions for different Reber strings.\n",
    "- `ufuncs.py` contains some utility functions used in the experiments like computing dot product of two matrices, computing false match probability for SDRs given some specific network parameters, etc.\n",
    "\n",
    "\n",
    "#### Description of Issues 001-005\n",
    "~~~\n",
    "Issue Description:\n",
    "------------\n",
    "Issue 001: \n",
    "    When a column bursts, but no (matching) dendrite with connections to the previous timestep's activity \n",
    "    are found AND when all HTM cells in a given minicolumn run out of their capacity to grow any new\n",
    "    dendrite (given by 'maxDendritesPerCell').\n",
    "    \t\n",
    "Issue 002:\n",
    "    When a dendrite has more synapses than its capacity given by 'maxSynapsesPerDendrite'.\n",
    "    \n",
    "Issue 003:\n",
    "    When multiple matching dendrites are found in a bursting column.\n",
    "    \n",
    "Issue 004:\n",
    "    To be read in the same context as Issue 001. See htm_net.py.\n",
    "    \n",
    "Issue 005:\n",
    "    This issue reports a fundamental flaw in the learning of SDRs. If the total number of cells with \n",
    "    permanence reinforcement on any one of their dendrites at any given timestep during execcution\n",
    "    falls below the set NMDA threshold of the network, issue 005 is reported at the output terminal.\n",
    "    It breaks the execution of the program for the current reber string and starts execution from the\n",
    "    next reber string in the input stream.\n",
    "    In the current implementation of HTM, this issue is generally found to be in 5% of the total\n",
    "    number of reber strings in the inputstream.\n",
    "~~~\n",
    "\n",
    "## Some Results\n",
    "#### Learning a Single Reber String _PVPVZ_\n",
    "\n",
    "#### Learning SRG with `maxDendriteDormancy` parameter set to `24*8`\n",
    "\n",
    "#### Learning SRG with `maxDendriteDormancy` parameter set to `inf`\n",
    "\n",
    "\n",
    "## Thesis (Full Text)\n",
    "The full thesis can be found [here](https://docs.google.com/document/d/10CVceFrXVdygoLiY0-jKl_dnbHttWX6Iyar5BQjbg8I/edit?usp=sharing).\n",
    "\n",
    "## Presentation Slides\n",
    "Presentation slides can be found [here](https://github.com/TaherHabib/sequence-learning-model/blob/master/Modelling%20Implicit%20Acquisition%20of%20Sequential%20Information%20Using%20a%20Neocortical%20Neural%20Network%20Hierarchical%20Temporal%20Memory.pdf)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
